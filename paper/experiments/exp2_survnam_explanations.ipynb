{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from survnam.nam import metrics\n",
    "from survnam.nam import data_utils\n",
    "from survnam.nam import *\n",
    "import sklearn.metrics\n",
    "from sksurv.util import Surv\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.nonparametric import nelson_aalen_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6  # \"Hyper-parameter: learning rate.\"\n",
    "l2_regularization = 0.00  # \"Hyper-parameter: l2 weight decay\"\n",
    "dropout = 0.0  # \"Hyper-parameter: Dropout rate\"\n",
    "feature_dropout = 0.0  # \"Hyper-parameter: Prob. with which features are dropped\"\n",
    "\n",
    "training_epochs = 20 # \"The number of epochs to run training for.\"\n",
    "batch_size = 1  # \"Hyper-parameter: batch size.\"\n",
    "seed = 42  # \"Seed used for reproducibility.\"\n",
    "n_basis_functions = 1000  # \"Number of basis functions to use in a FeatureNN for a real-valued feature.\"\n",
    "units_multiplier = 2  # \"Number of basis functions for a categorical feature\"\n",
    "\n",
    "hidden_units = []  # \"Amounts of neurons for additional hidden layers, e.g. 64,32,32\"\n",
    "log_file = \"survnam.log\"  # \"File where to store summaries.\"\n",
    "shallow_layer = \"exu\"  # \"Activation function used for the first layer: (1) relu, (2) exu\"\n",
    "hidden_layer = \"relu\"  # \"Activation function used for the hidden layers: (1) relu, (2) exu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, times, nelson_est):\n",
    "    pbar = tqdm.tqdm(enumerate(data_loader, start=1), total=len(data_loader))\n",
    "    total_loss = 0\n",
    "    for i, (xk, chf_k, weight_k) in pbar:\n",
    "        xk = torch.unsqueeze(xk, dim=0)\n",
    "        xk = xk\n",
    "        chf_k = chf_k\n",
    "        weight_k = weight_k\n",
    "        logits, _ = model.forward(xk)\n",
    "        loss = criterion(logits, chf_k, times, nelson_est, weight_k)\n",
    "        loss.backward(retain_graph=True)\n",
    "        x_loss = loss.item()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        total_loss += x_loss\n",
    "    pbar.set_description(f\"train | loss = {total_loss:.5f}\")\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, chfs, weights, device, times, nelson_est):\n",
    "    times = torch.tensor(times, device = device)\n",
    "    nelson_est = torch.tensor(nelson_est, device = device)\n",
    "    model = NeuralAdditiveModel(\n",
    "        input_size=x_train.shape[-1],\n",
    "        shallow_units=data_utils.calculate_n_units(x_train, n_basis_functions, units_multiplier),\n",
    "        hidden_units=list(map(int, hidden_units)),\n",
    "        shallow_layer=ExULayer if shallow_layer == \"exu\" else ReLULayer,\n",
    "        hidden_layer=ExULayer if hidden_layer == \"exu\" else ReLULayer,\n",
    "        hidden_dropout=dropout,\n",
    "        feature_dropout=feature_dropout).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "\n",
    "    criterion = metrics.survnam_loss\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, device=device), \n",
    "                                  torch.tensor(chfs, device=device), \n",
    "                                  torch.tensor(weights, device=device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    best_weights = None  # to store the optimal performance\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        model = model.train()  # training the base\n",
    "        total_loss = train_one_epoch(model, criterion, optimizer, train_loader, device, times, nelson_est)\n",
    "        # record the log of training (training loss)\n",
    "        logging.info(f\"epoch {epoch} | train | {total_loss}\")\n",
    "        scheduler.step()  # update the learning rate\n",
    "        best_weights = copy.deepcopy(model.state_dict())  # update the optimal base\n",
    "    model.load_state_dict(best_weights)  # continue training from the optimal base\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd036fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)  # random seed\n",
    "handlers = [logging.StreamHandler()]\n",
    "if log_file:\n",
    "    handlers.append(logging.FileHandler(log_file))\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(message)s\", handlers=handlers)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "logging.info(\"load data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanation(x, survnam):\n",
    "    return survnam(torch.transpose(torch.atleast_3d(torch.tensor(x.astype(\"float32\"))), 1, 2).to(device))[1].detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dataset0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0_train = pd.read_csv(\"../data/exp2_dataset0_train.csv\")\n",
    "dataset0_test = pd.read_csv(\"../data/exp2_dataset0_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset0_train.iloc[:, :5]\n",
    "X_test = dataset0_test.iloc[:, :5]\n",
    "y_train = Surv.from_dataframe(\"event\", \"time\", dataset0_train)\n",
    "y_test = Surv.from_dataframe(\"event\", \"time\", dataset0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(n_estimators=150, max_depth=12, max_features=3, min_samples_leaf=6, min_samples_split=10, random_state=123)\n",
    "rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SurvNAM article\n",
    "sds = np.array(0.05 * (X_test.describe().loc[\"max\"] - X_test.describe().loc[\"min\"]))\n",
    "def generate_neighbours(ind):\n",
    "    x = X_test.iloc[ind].values\n",
    "    neighbours = np.random.multivariate_normal(x, np.diag(sds**2), 1000)\n",
    "    neighbours[0, ] = x\n",
    "    return neighbours\n",
    "    \n",
    "def get_weights(neighbourhood, distance_metric=\"euclidean\"):\n",
    "    distances = sklearn.metrics.pairwise_distances(\n",
    "            neighbourhood,\n",
    "            neighbourhood[0].reshape(1, -1),\n",
    "            metric=distance_metric,\n",
    "        ).ravel()\n",
    "    weights = np.exp(-(distances**2) / 0.5).squeeze()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_explanation(model, data, y):\n",
    "    event_field, time_field = y.dtype.names\n",
    "    nelson_est = nelson_aalen_estimator(y[event_field], y[time_field])\n",
    "    times = nelson_est[0][0:-1]\n",
    "    nelson_est = nelson_est[1][0:-1]\n",
    "    preds = model.predict_cumulative_hazard_function(data)\n",
    "    chfs = (\n",
    "            np.array([chf(times) for chf in preds]) + 1e-32 \n",
    "        )\n",
    "    times_to_provide = np.hstack((times, [times[-1] + 1e-10]))\n",
    "    return chfs, times_to_provide, nelson_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(X_test)\n",
    "survnam_explanations = [None for i in range(n_obs)]\n",
    "N = 1000\n",
    "distance_metric=\"euclidean\"\n",
    "for i, obs in tqdm.tqdm(enumerate(X_test.values)):\n",
    "    neighbourhood = generate_neighbours(i)\n",
    "    weights = get_weights(neighbourhood, distance_metric=distance_metric)\n",
    "    chfs, times_to_provide, nelson_est = data_to_explanation(rsf, neighbourhood, y_train)\n",
    "    survnam_model = train_model(neighbourhood.astype(\"float32\"), chfs, \n",
    "                weights, device, times_to_provide, nelson_est)\n",
    "    survnam_explanations[i] = get_explanation(obs, survnam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(survnam_explanations).to_csv(\"exp2_survnam_explanations_dataset0_rsf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHSurvivalAnalysis()\n",
    "cph.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(X_test)\n",
    "survnam_explanations_dataset0_cph = [None for i in range(n_obs)]\n",
    "N = 1000\n",
    "distance_metric=\"euclidean\"\n",
    "for i, obs in tqdm.tqdm(enumerate(X_test.values)):\n",
    "    neighbourhood = generate_neighbours(i)\n",
    "    weights = get_weights(neighbourhood, distance_metric=distance_metric)\n",
    "    chfs, times_to_provide, nelson_est = data_to_explanation(cph, neighbourhood, y_train)\n",
    "    survnam_model = train_model(neighbourhood.astype(\"float32\"), chfs, \n",
    "                weights, device, times_to_provide, nelson_est)\n",
    "    survnam_explanations_dataset0_cph[i] = get_explanation(obs, survnam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(survnam_explanations_dataset0_cph).to_csv(\"exp2_survnam_explanations_dataset0_cph.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dataset1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_train = pd.read_csv(\"../data/exp2_dataset1_train.csv\")\n",
    "dataset1_test = pd.read_csv(\"../data/exp2_dataset1_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset1_train.iloc[:, :5]\n",
    "X_test = dataset1_test.iloc[:, :5]\n",
    "y_train = Surv.from_dataframe(\"event\", \"time\", dataset1_train)\n",
    "y_test = Surv.from_dataframe(\"event\", \"time\", dataset1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(n_estimators=150, max_depth=12, max_features=3, min_samples_leaf=6, min_samples_split=10, random_state=123)\n",
    "rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SurvNAM article\n",
    "sds = np.array(0.05 * (X_test.describe().loc[\"max\"] - X_test.describe().loc[\"min\"]))\n",
    "def generate_neighbours(ind):\n",
    "    x = X_test.iloc[ind].values\n",
    "    neighbours = np.random.multivariate_normal(x, np.diag(sds**2), 1000)\n",
    "    neighbours[0, ] = x\n",
    "    return neighbours\n",
    "\n",
    "def get_weights(neighbourhood, distance_metric=\"euclidean\"):\n",
    "    distances = sklearn.metrics.pairwise_distances(\n",
    "            neighbourhood,\n",
    "            neighbourhood[0].reshape(1, -1),\n",
    "            metric=distance_metric,\n",
    "        ).ravel()\n",
    "    weights = np.exp(-(distances**2) / 0.5).squeeze()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(X_test)\n",
    "survnam_explanations_dataset1_rsf = [None for i in range(n_obs)]\n",
    "N = 1000\n",
    "distance_metric=\"euclidean\"\n",
    "for i, obs in tqdm.tqdm(enumerate(X_test.values)):\n",
    "    neighbourhood = generate_neighbours(i)\n",
    "    weights = get_weights(neighbourhood, distance_metric=distance_metric)\n",
    "    chfs, times_to_provide, nelson_est = data_to_explanation(rsf, neighbourhood, y_train)\n",
    "    survnam_model = train_model(neighbourhood.astype(\"float32\"), chfs, \n",
    "                weights, device, times_to_provide, nelson_est)\n",
    "    survnam_explanations_dataset1_rsf[i] = get_explanation(obs, survnam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(survnam_explanations_dataset1_rsf).to_csv(\"exp2_survnam_explanations_dataset1_rsf.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHSurvivalAnalysis()\n",
    "cph.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(X_test)\n",
    "survnam_explanations_dataset1_cph = [None for i in range(n_obs)]\n",
    "N = 1000\n",
    "distance_metric=\"euclidean\"\n",
    "for i, obs in tqdm.tqdm(enumerate(X_test.values)):\n",
    "    neighbourhood = generate_neighbours(i)\n",
    "    weights = get_weights(neighbourhood, distance_metric=distance_metric)\n",
    "    chfs, times_to_provide, nelson_est = data_to_explanation(cph, neighbourhood, y_train)\n",
    "    survnam_model = train_model(neighbourhood.astype(\"float32\"), chfs, \n",
    "                weights, device, times_to_provide, nelson_est)\n",
    "    survnam_explanations_dataset1_cph[i] = get_explanation(obs, survnam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(survnam_explanations_dataset1_cph).to_csv(\"exp2_survnam_explanations_dataset1_cph.csv\",  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
